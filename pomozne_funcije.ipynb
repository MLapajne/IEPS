{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pomožne funkcije pri izdelavi pajka\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disallow: /iskanje\n",
      "Disallow: /bin/\n",
      "Disallow: /lbin/\n",
      "Disallow: /adserver/\n",
      "Disallow: *modal:*\n",
      "Crawl-delay: 2\n",
      "{'User-agent': {'Disallow': ['/iskanje', '/bin/', '/lbin/', '/adserver/', '*modal:*'], 'Allow': []}, 'Crawl-delay': '2', 'Sitemap': ['https://www.24ur.com/sitemaps/sites/10010']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "def robots_context(text):\n",
    "    result_dict = {}\n",
    "    lines = text.split('\\n')\n",
    "    sitemaps = []\n",
    "    for line in lines:\n",
    "        if line.startswith('Sitemap:'):\n",
    "            if len(line.split())>1: sitemaps.append(line.split()[1])\n",
    "        elif line.startswith('User-agent:') and line.split()[1] == '*':\n",
    "            disallows = []\n",
    "            allows = []\n",
    "            for agent in lines[lines.index(line)+1:]:\n",
    "                print(agent)\n",
    "                if agent.strip() and  not agent.strip().startswith(('Disallow:', 'Allow:', '#')):\n",
    "                    break\n",
    "                if agent.startswith('Disallow:'):\n",
    "                    if len(agent.split())>1: disallows.append(agent.split()[1])        \n",
    "                if agent.startswith('Allow:'):\n",
    "                    if len(agent.split())>1: allows.append(agent.split()[1]) \n",
    "            result_dict['User-agent'] = {'Disallow': disallows, 'Allow': allows}\n",
    "        elif line.startswith('Crawl-delay:'):\n",
    "            if len(line.split())>1: result_dict['Crawl-delay'] = line.split()[1]\n",
    "        else:\n",
    "            continue\n",
    "    result_dict['Sitemap'] = sitemaps\n",
    "    return result_dict\n",
    "\n",
    "def main_page():\n",
    "    #vstavimo neko stran ki ni prva.\n",
    "\n",
    "    #kliče funkcijo, ki scrapa stran\n",
    "\n",
    "    #pridobi osnovno stran in vstavi dobljene info v bazo\n",
    "\n",
    "    #na izhodu dobimo:\n",
    "    #id glavne -> mogoče samo to vrne, ker se lahko kliče bazo\n",
    "    #vse strani iz sitemap\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def url_sitemap(url):\n",
    "    #za vstavlen url osnovne domene dobi vse povezave na strani\n",
    "    response = requests.get(url + '/robots.txt')\n",
    "    sitemap_list = robots_context(response.text)\n",
    "    print(sitemap_list['Sitemap'])\n",
    "    urls = []\n",
    "    for sitemap in sitemap_list['Sitemap']:\n",
    "        urls.extend(parse_sitemap(sitemap))\n",
    "    return urls\n",
    "\n",
    "\n",
    "def parse_sitemap(url_sitemap):\n",
    "    #dobi urlje od sitmap xml datoteke\n",
    "    urls = []\n",
    "\n",
    "    #če je ni\n",
    "    if url_sitemap is  None:\n",
    "        return []\n",
    "\n",
    "    # Fetch the sitemap XML file\n",
    "    response = requests.get(url_sitemap)\n",
    "    if response.status_code == 200:\n",
    "        #preoblikujemo v leš\n",
    "        soup = BeautifulSoup(response.content, 'xml')\n",
    "        \n",
    "\n",
    "    \n",
    "        #išče <sitemap>, ki nakazuje na .xml datoteke\n",
    "        sitemap = soup.find('sitemap')\n",
    "        if sitemap is not None:\n",
    "            \n",
    "            \n",
    "            for sitemap in soup.sitemapindex.find_all('sitemap'):\n",
    "                # Extract the URLs from the sitemap\n",
    "                loc = sitemap.loc.text\n",
    "                \n",
    "                #print(f'Location: {loc}')\n",
    "\n",
    "                #če so xml datoteke\n",
    "                if loc.endswith(\".xml\") or loc.endswith(\".XML\"):\n",
    "                    urls.extend(parse_sitemap(loc))\n",
    "                    \n",
    "            #kliče sebe s parametrom, ki vsebuje ime xml datoteke z url-ji\n",
    "        \n",
    "        else:\n",
    "            #datotka vsebuje url-je\n",
    "            urlset = soup.find('urlset')\n",
    "            if urlset is not None:\n",
    "                url = soup.find('url')\n",
    "                if url is not None:\n",
    "                    urls.extend([element.text for element in soup.find_all('loc')])\n",
    "       \n",
    "  \n",
    "    return urls\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "url = 'https://okusno.je/'  # Replace example.com with the actual domain\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code == 200:  # Check if the response is successful\n",
    "    #print(len(url_sitemap(url)))\n",
    "    resp = requests.get(url + '/robots.txt', headers=headers)\n",
    "\n",
    "    print(robots_context(resp.text))\n",
    "else:\n",
    "    print('Failed to retrieve robots.txt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs.python.org/3/library\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "websites = set()\n",
    "web = \"https://docs.python.org/3/library/\"\n",
    "\n",
    "# Odstrani duplikate glede na url:\n",
    "#    HTTP vs HTTPS\n",
    "#    www vs non-www\n",
    "#    Trailing slash: example.com vs example.com/\n",
    "def is_unique(website):\n",
    "    parsed = urlparse(website)\n",
    "    #odstrani zadnji / če obstaja\n",
    "    path = parsed.path[:-1] if (parsed.path[-1] == \"/\") else parsed.path\n",
    "    #docs.python.org/3/library\n",
    "    url = parsed.hostname + path\n",
    "    #primerja, če že obstaja obtoječa stran\n",
    "    if url in websites:\n",
    "        return False\n",
    "    websites.add(url)\n",
    "    print(url)\n",
    "    return True\n",
    "\n",
    "is_unique(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sivar/nltk_data',\n",
       " 'c:\\\\Users\\\\sivar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data',\n",
       " 'c:\\\\Users\\\\sivar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data',\n",
       " 'c:\\\\Users\\\\sivar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data',\n",
       " 'C:\\\\Users\\\\sivar\\\\AppData\\\\Roaming\\\\nltk_data',\n",
       " 'C:\\\\nltk_data',\n",
       " 'D:\\\\nltk_data',\n",
       " 'E:\\\\nltk_data']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib \n",
    "from bs4 import BeautifulSoup\n",
    "#pip install bs4\n",
    "from langdetect import detect\n",
    "#pip install langdetect\n",
    "from nltk.corpus import stopwords\n",
    "#pip install nltk\n",
    "#python -m nltk.downloader stopwords or nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from iso639 import languages\n",
    "#pip install iso-639\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slovenian\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\sivar\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\slovenian'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m url_address1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttp://www.google.com\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m url_address2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttp://www.google.com/search\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 94\u001b[0m jaccard_similarity(url_address1, url_address2)\n",
      "Cell \u001b[1;32mIn[7], line 75\u001b[0m, in \u001b[0;36mjaccard_similarity\u001b[1;34m(url1, url2)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39mCalculate the Jaccard similarity coefficient between the text content of two websites.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m# Get the text content of each website\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m text1 \u001b[39m=\u001b[39m get_website_text(url1)\n\u001b[0;32m     76\u001b[0m text2 \u001b[39m=\u001b[39m get_website_text(url2)\n\u001b[0;32m     78\u001b[0m \u001b[39m# Convert the text content to sets of words\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m, in \u001b[0;36mget_website_text\u001b[1;34m(url_address)\u001b[0m\n\u001b[0;32m     55\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(sentence)\n\u001b[0;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m lang \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mother\u001b[39m\u001b[39m\"\u001b[39m:    \n\u001b[1;32m---> 57\u001b[0m     filtered_words \u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m w \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39mwords(lang)]\n\u001b[0;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     filtered_words \u001b[39m=\u001b[39m  [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens]\n",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     55\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(sentence)\n\u001b[0;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m lang \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mother\u001b[39m\u001b[39m\"\u001b[39m:    \n\u001b[1;32m---> 57\u001b[0m     filtered_words \u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m w \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39;49mwords(lang)]\n\u001b[0;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     filtered_words \u001b[39m=\u001b[39m  [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens]\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[39m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(f) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[39m.\u001b[39mappend(fp\u001b[39m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root\u001b[39m.\u001b[39;49mjoin(file)\u001b[39m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\u001b[39mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decorator\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[39m=\u001b[39m (args[\u001b[39m0\u001b[39m], add_py3_data(args[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m args[\u001b[39m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m init_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:312\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(_path)\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(_path):\n\u001b[1;32m--> 312\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m _path\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\sivar\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\slovenian'"
     ]
    }
   ],
   "source": [
    "def recognizeLang(text):\n",
    "    \"\"\"\n",
    "    Pomožna funkcija, ki pretvori pravilen zapis jezika\n",
    "    \"\"\"\n",
    "    detLang = detect(text)\n",
    "    \n",
    "    if detLang == 'el':\n",
    "        lang = \"greek\"\n",
    "    elif detLang == 'zh-cn':\n",
    "        lang = \"chinese\"\n",
    "    elif detLang == 'zh-tw':\n",
    "        lang = \"chinese\"\n",
    "    else:\n",
    "        lang = languages.get(alpha2=detLang).name.lower()\n",
    "    return lang\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_website_text(url_address):\n",
    "    \"\"\"\n",
    "    Funkcija prebere besedilo strani in ga reducira na najmanjše možno obliko\n",
    "    \"\"\"\n",
    "\n",
    "    with urllib.request.urlopen(url_address) as url:\n",
    "        html = url.read()\n",
    "        \n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "    sentence = text.lower()\n",
    "   \n",
    "    # poišče jezik vnešenega teksta iz prvih 100 besed\n",
    "    reduced_sent = \" \".join(sentence.split()[:100])\n",
    "    lang = recognizeLang(reduced_sent)\n",
    "    print(lang)\n",
    "\n",
    "    # creates tokens, removes stopwords and punctation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    if not lang == \"other\":    \n",
    "        filtered_words = [w for w in tokens if not w in stopwords.words(lang)]\n",
    "    else:\n",
    "        filtered_words =  [w for w in tokens]\n",
    "\n",
    "    # odstrani podvojitve\n",
    "    reduced_text = list(set(filtered_words))\n",
    "    #print(reduced_text)\n",
    "\n",
    "    return \" \".join(reduced_text)\n",
    "\n",
    "\n",
    "\n",
    "# dobljena s pomočjo chatGPT\n",
    "def jaccard_similarity(url1, url2):\n",
    "    \"\"\"\n",
    "    Calculate the Jaccard similarity coefficient between the text content of two websites.\n",
    "    \"\"\"\n",
    "    # Get the text content of each website\n",
    "    text1 = get_website_text(url1)\n",
    "    text2 = get_website_text(url2)\n",
    "    \n",
    "    # Convert the text content to sets of words\n",
    "    words1 = set(text1.split())\n",
    "    words2 = set(text2.split())\n",
    "    \n",
    "    # Calculate the Jaccard similarity coefficient\n",
    "    intersection = len(words1.intersection(words2))\n",
    "    union = len(words1.union(words2))\n",
    "    jaccard_similarity = intersection / union\n",
    "    \n",
    "    return jaccard_similarity\n",
    "\n",
    "\n",
    "\n",
    "# This is an example of code which extracts all text from a web page and compares its content\n",
    "url_address1 = \"http://www.google.com\"\n",
    "url_address2 = \"http://www.google.com/search\"\n",
    "jaccard_similarity(url_address1, url_address2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\sivar\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\slovenian'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m\"\u001b[39;49m\u001b[39mslovenian\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[39m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(f) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[39m.\u001b[39mappend(fp\u001b[39m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root\u001b[39m.\u001b[39;49mjoin(file)\u001b[39m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\u001b[39mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decorator\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[39m=\u001b[39m (args[\u001b[39m0\u001b[39m], add_py3_data(args[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m args[\u001b[39m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m init_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sivar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:312\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(_path)\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(_path):\n\u001b[1;32m--> 312\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m _path\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\sivar\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\slovenian'"
     ]
    }
   ],
   "source": [
    "stopwords.words(\"slovenian\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8849346f80767a92e2c33b323c45271d0753cfa3d5d4c78243f14c0b566205e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
