{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pomožne funkcije pri izdelavi pajka\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disallow: /iskanje\n",
      "Disallow: /bin/\n",
      "Disallow: /lbin/\n",
      "Disallow: /adserver/\n",
      "Disallow: *modal:*\n",
      "Crawl-delay: 2\n",
      "{'User-agent': {'Disallow': ['/iskanje', '/bin/', '/lbin/', '/adserver/', '*modal:*'], 'Allow': []}, 'Crawl-delay': '2', 'Sitemap': ['https://www.24ur.com/sitemaps/sites/10010']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "def robots_context(text):\n",
    "    result_dict = {}\n",
    "    lines = text.split('\\n')\n",
    "    sitemaps = []\n",
    "    for line in lines:\n",
    "        if line.startswith('Sitemap:'):\n",
    "            if len(line.split())>1: sitemaps.append(line.split()[1])\n",
    "        elif line.startswith('User-agent:') and line.split()[1] == '*':\n",
    "            disallows = []\n",
    "            allows = []\n",
    "            for agent in lines[lines.index(line)+1:]:\n",
    "                print(agent)\n",
    "                if agent.strip() and  not agent.strip().startswith(('Disallow:', 'Allow:', '#')):\n",
    "                    break\n",
    "                if agent.startswith('Disallow:'):\n",
    "                    if len(agent.split())>1: disallows.append(agent.split()[1])        \n",
    "                if agent.startswith('Allow:'):\n",
    "                    if len(agent.split())>1: allows.append(agent.split()[1]) \n",
    "            result_dict['User-agent'] = {'Disallow': disallows, 'Allow': allows}\n",
    "        elif line.startswith('Crawl-delay:'):\n",
    "            if len(line.split())>1: result_dict['Crawl-delay'] = line.split()[1]\n",
    "        else:\n",
    "            continue\n",
    "    result_dict['Sitemap'] = sitemaps\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "\n",
    "def url_sitemap(url):\n",
    "    #za vstavlen url osnovne domene dobi vse povezave na strani\n",
    "    response = requests.get(url + '/robots.txt')\n",
    "    sitemap_list = robots_context(response.text)\n",
    "    print(sitemap_list['Sitemap'])\n",
    "    urls = []\n",
    "    for sitemap in sitemap_list['Sitemap']:\n",
    "        urls.extend(parse_sitemap(sitemap))\n",
    "    return urls\n",
    "\n",
    "\n",
    "def parse_sitemap(url_sitemap):\n",
    "    #dobi urlje od sitmap xml datoteke\n",
    "    urls = []\n",
    "\n",
    "    #če je ni\n",
    "    if url_sitemap is  None:\n",
    "        return []\n",
    "\n",
    "    # Fetch the sitemap XML file\n",
    "    response = requests.get(url_sitemap)\n",
    "    if response.status_code == 200:\n",
    "        #preoblikujemo v leš\n",
    "        soup = BeautifulSoup(response.content, 'xml')\n",
    "        \n",
    "\n",
    "    \n",
    "        #išče <sitemap>, ki nakazuje na .xml datoteke\n",
    "        sitemap = soup.find('sitemap')\n",
    "        if sitemap is not None:\n",
    "            \n",
    "            \n",
    "            for sitemap in soup.sitemapindex.find_all('sitemap'):\n",
    "                # Extract the URLs from the sitemap\n",
    "                loc = sitemap.loc.text\n",
    "                \n",
    "                #print(f'Location: {loc}')\n",
    "\n",
    "                #če so xml datoteke\n",
    "                if loc.endswith(\".xml\") or loc.endswith(\".XML\"):\n",
    "                    urls.extend(parse_sitemap(loc))\n",
    "                    \n",
    "            #kliče sebe s parametrom, ki vsebuje ime xml datoteke z url-ji\n",
    "        \n",
    "        else:\n",
    "            #datotka vsebuje url-je\n",
    "            urlset = soup.find('urlset')\n",
    "            if urlset is not None:\n",
    "                url = soup.find('url')\n",
    "                if url is not None:\n",
    "                    urls.extend([element.text for element in soup.find_all('loc')])\n",
    "       \n",
    "  \n",
    "    return urls\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "url = 'https://okusno.je/'  # Replace example.com with the actual domain\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code == 200:  # Check if the response is successful\n",
    "    #print(len(url_sitemap(url)))\n",
    "    resp = requests.get(url + '/robots.txt', headers=headers)\n",
    "\n",
    "    print(robots_context(resp.text))\n",
    "else:\n",
    "    print('Failed to retrieve robots.txt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs.python.org/3/library\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "websites = set()\n",
    "web = \"https://docs.python.org/3/library/\"\n",
    "\n",
    "# Odstrani duplikate glede na url:\n",
    "#    HTTP vs HTTPS\n",
    "#    www vs non-www\n",
    "#    Trailing slash: example.com vs example.com/\n",
    "def is_unique(website):\n",
    "    parsed = urlparse(website)\n",
    "    #odstrani zadnji / če obstaja\n",
    "    path = parsed.path[:-1] if (parsed.path[-1] == \"/\") else parsed.path\n",
    "    #docs.python.org/3/library\n",
    "    url = parsed.hostname + path\n",
    "    #primerja, če že obstaja obtoječa stran\n",
    "    if url in websites:\n",
    "        return False\n",
    "    websites.add(url)\n",
    "    print(url)\n",
    "    return True\n",
    "\n",
    "is_unique(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application/vnd.openxmlformats-officedocument.wordprocessingml.document\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "url = \"https://www.example.com/path/to/page.docx\"\n",
    "\n",
    "import mimetypes\n",
    "mimetype,encoding = mimetypes.guess_type(url)\n",
    "mimetype and mimetype.startswith('image')\n",
    "print(mimetype)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib \n",
    "from bs4 import BeautifulSoup\n",
    "#pip install bs4\n",
    "from langdetect import detect\n",
    "#pip install langdetect\n",
    "import string\n",
    "from stopwordsiso import stopwords\n",
    "import re\n",
    "from datasketch import MinHash\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognizeLang(text):\n",
    "    \"\"\"\n",
    "    Pomožna funkcija, ki pretvori pravilen zapis jezika\n",
    "    \"\"\"\n",
    "    detLang = detect(text)\n",
    "    \n",
    "\n",
    "    if detLang == 'zh-cn':\n",
    "        lang = \"zh\"\n",
    "    elif detLang == 'zh-tw':\n",
    "        lang = \"zh\"\n",
    "    else:\n",
    "        lang = detLang\n",
    "    return lang\n",
    "\n",
    "\n",
    "\n",
    "def get_website_text(url_address):\n",
    "    \"\"\"\n",
    "    Funkcija prebere besedilo strani in ga reducira na najmanjše možno obliko\n",
    "    \"\"\"\n",
    "\n",
    "    with urllib.request.urlopen(url_address) as url:\n",
    "        html = url.read()\n",
    "        \n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    raw_text = soup.get_text().strip().lower()\n",
    "    # remove unwanted characters such as punctuation marks or special symbols\n",
    "    raw_text = re.sub(r'[^\\w\\s]', '', raw_text)\n",
    "    word_list = []\n",
    "    for word in raw_text.split():\n",
    "        word_list.append(word)\n",
    "        \n",
    "\n",
    "    # poišče jezik vnešenega teksta iz prvih 100 besed\n",
    "    reduced_sent = \" \".join(word_list[:100])\n",
    "    lang = recognizeLang(reduced_sent)\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "    if lang == \"th\" or lang == \"zh\" or lang == \"vi\" or lang == \"ja\" or lang == \"ko\":\n",
    "        #azijci ne delajo presledkov, zato je potrebno obravnavati vsako črko posebej\n",
    "        asian_words = [c for w in word_list for c in w if not c in stopwords(lang)]\n",
    "        regex = re.compile('[a-zA-Z]')\n",
    "        filtered_words = [word for word in asian_words if not regex.search(word)]\n",
    "    elif not lang == \"other\":    \n",
    "        filtered_words = [w for w in word_list if not w in stopwords(lang)]    \n",
    "    else:\n",
    "        filtered_words = word_list\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "\n",
    "def generate_minhash(text_list):\n",
    "    m = MinHash()\n",
    "    for text in text_list:\n",
    "        m.update(text.encode('utf8'))\n",
    "        hashvalues = m.hashvalues\n",
    "    return hashvalues\n",
    "\n",
    "def is_page_similar(m1, m2):\n",
    "    return float(np.count_nonzero(m1==m2)) / float(len(m1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3046875"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1 = get_website_text(\"https://github.com/kayzhu/LSHash\")\n",
    "hash1 = generate_minhash(words1)\n",
    "words2 = get_website_text(\"https://github.com/kayzhu/LSHash/pulls\")\n",
    "hash2 = generate_minhash(words2)\n",
    "is_page_similar(hash1, hash2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disallow: /iskanje\n",
      "Disallow: /bin/\n",
      "Disallow: /lbin/\n",
      "Disallow: /adserver/\n",
      "Disallow: *modal:*\n",
      "Crawl-delay: 2\n",
      "{'User-agent': {'Disallow': ['/iskanje', '/bin/', '/lbin/', '/adserver/', '*modal:*'], 'Allow': []}, 'Crawl-delay': '2', 'Sitemap': ['https://www.24ur.com/sitemaps/sites/10010']}\n"
     ]
    }
   ],
   "source": [
    "#pomembno ker nekateri strani brez tega ne dovolijo dostop!\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "url = 'https://okusno.je/'  # Replace example.com with the actual domain\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code == 200:  # Check if the response is successful\n",
    "    #print(len(url_sitemap(url))) #strani je preveč da bi jih editor izpisal zato len\n",
    "    resp = requests.get(url + '/robots.txt', headers=headers)\n",
    "    print(robots_context(resp.text))\n",
    "else:\n",
    "    print('Failed to retrieve robots.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8849346f80767a92e2c33b323c45271d0753cfa3d5d4c78243f14c0b566205e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
