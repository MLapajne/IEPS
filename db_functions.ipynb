{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these are not optimized (a new connection is created every time) but should work fine for initial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserting values into site\n"
     ]
    }
   ],
   "source": [
    "# insert data for site\n",
    "def insert_site_data(domain, robots_content, crawl_delay, sitemap_content):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    print(\"\\nInserting values into site\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT into crawldb.site (domain, robots_content, crawl_delay, sitemap_content) \\\n",
    "                 VALUES (%s, %s, %s, %s)\", (domain, robots_content, crawl_delay, sitemap_content))\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()    \n",
    "\n",
    "# testing\n",
    "# insert_site_data(\"http://bbbb.si\", \"robots content string\", 5, \"sitemap content string\")\n",
    "# insert_site_data(\"http://ccccc.si\", \"robots content string\", 2, \"sitemap content string\")\n",
    "insert_site_data(\"http://ee.si\", \"robots content string\", 5.5, \"sitemap content string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting crawl delay\n",
      "5.5\n"
     ]
    }
   ],
   "source": [
    "# returns crawl delay (float) for specified domain url\n",
    "def get_crawl_delay(domain_url):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    print(\"Getting crawl delay\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # get url of first page with code set to FRONTIER\n",
    "    cur.execute(\"SELECT crawl_delay FROM crawldb.site WHERE domain = %s\", (domain_url,))\n",
    "    page_url = cur.fetchone()[0]\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return page_url\n",
    "\n",
    "print(get_crawl_delay(\"http://ddd.si\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting robots.txt content\n",
      "robots content string\n"
     ]
    }
   ],
   "source": [
    "# returns robots.txt content\n",
    "def get_robots_content(domain_url):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    print(\"Getting robots.txt content\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # get url of first page with code set to FRONTIER\n",
    "    cur.execute(\"SELECT robots_content FROM crawldb.site WHERE domain = %s\", (domain_url,))\n",
    "    robots_content = cur.fetchone()[0]\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return robots_content\n",
    "\n",
    "print(get_robots_content(\"http://bbbb.si\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting site ID\n",
      "Site ID: 5\n",
      "Inserting into frontier: 5\n",
      "Getting site ID\n",
      "Site ID: 5\n",
      "Inserting into frontier: 5\n",
      "Getting site ID\n",
      "Site ID: 5\n",
      "Inserting into frontier: 5\n",
      "Getting site ID\n",
      "Site ID: 5\n",
      "Inserting into frontier: 5\n"
     ]
    }
   ],
   "source": [
    "# insert url into frontier - inserts into table 'page' with the page_type_code value set to FRONTIER\n",
    "def insert_page_into_frontier(domain, url):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    print(\"Getting site ID\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # get id of domain url\n",
    "    cur.execute(\"SELECT id FROM crawldb.site WHERE domain= %s\", (domain,)) \n",
    "    \n",
    "    # TODO check if domain is stored and return err?\n",
    "    site_id_result = cur.fetchone()\n",
    "    if(site_id_result is None):\n",
    "        print(\"No domain stored with this url\")\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    site_id = site_id_result[0]\n",
    "    \n",
    "    print(\"Site ID: {}\".format(site_id))\n",
    "\n",
    "    # check if there is already an existing url in the database or frontier\n",
    "    cur.execute(\"SELECT id FROM crawldb.page WHERE url = %s\", (url,))\n",
    "    \n",
    "    id_of_original = cur.fetchone()\n",
    "\n",
    "    if(id_of_original is None):\n",
    "         print(\"Inserting into frontier: {}\".format(site_id))\n",
    "         cur.execute(\"INSERT into crawldb.page (site_id, url, page_type_code) VALUES (%s, %s, 'FRONTIER')\", (site_id, url))\n",
    "    else:\n",
    "         print(\"Exsisting url already in database/frontier\")\n",
    "         # TODO what to do here ?\n",
    "\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# testing\n",
    "# insert_page_into_frontier(\"http://bbbb.si\", \"https://bbbb.si/page2\")\n",
    "# insert_page_into_frontier(\"http://bbbb.si\", \"https://bbbb.si/page3\")\n",
    "# insert_page_into_frontier(\"http://ccccc.si\", \"http://ccccc.si/page1\")\n",
    "# insert_page_into_frontier(\"http://ccccc.si\", \"http://ccccc.si/page2\")\n",
    "insert_page_into_frontier(\"http://ee.si\", \"http://ee.si/binary1\")\n",
    "insert_page_into_frontier(\"http://ee.si\", \"http://ee.si/binary2\")\n",
    "insert_page_into_frontier(\"http://ee.si\", \"http://ee.si/binary3\")\n",
    "insert_page_into_frontier(\"http://ee.si\", \"http://ee.si/binary4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking duplicate\n",
      "(14,)\n",
      "\n",
      "Inserting page as DUPLICATE\n"
     ]
    }
   ],
   "source": [
    "# when page is crawled, update table 'page' with obtained data\n",
    "# the function checks for duplicates\n",
    "def update_page_data(url, page_type_code, html_content, content_hash, http_status_code, accessed_time):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    print(\"Checking duplicate\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # check if there is a duplicate of content_hash\n",
    "    cur.execute(\"SELECT id FROM crawldb.page WHERE content_hash = %s AND html_content IS NOT NULL\", \\\n",
    "                 (content_hash,)) # NOTE will we store the hash of duplicates or is there no need?\n",
    "    original_site = cur.fetchone()\n",
    "    print(original_site)\n",
    "\n",
    "    # if duplicate exists\n",
    "    if original_site is not None:\n",
    "        page_type_code = 'DUPLICATE'\n",
    "        original_site_id = original_site[0]\n",
    "        print(\"\\nInserting page as DUPLICATE\")\n",
    "\n",
    "        # duplicates should have empty html_content column\n",
    "        cur.execute(\"UPDATE crawldb.page SET page_type_code= %s, content_hash = %s, http_status_code= %s, accessed_time= %s \\\n",
    "                     WHERE url= %s RETURNING id\", ('DUPLICATE', content_hash, http_status_code, accessed_time, url))\n",
    "        \n",
    "        # NOTE - this can cause an error if the page hasn't been inserted yet\n",
    "        id_of_updated_row = cur.fetchone()[0]\n",
    "        \n",
    "        # create link from duplicate pointing to 'original' page\n",
    "        # column names: from_page -> id of duplicate, to_page -> id of original\n",
    "        cur.execute(\"INSERT INTO crawldb.link (from_page, to_page) VALUES (%s, %s)\", (id_of_updated_row, original_site_id))\n",
    "\n",
    "\n",
    "    # if there are no duplicates\n",
    "    else:\n",
    "        print(\"\\nInserting values into page as {}\".format(page_type_code))\n",
    "        cur.execute(\"UPDATE crawldb.page SET page_type_code= %s, html_content= %s, content_hash = %s, http_status_code= %s, accessed_time= %s \\\n",
    "                WHERE url= %s\", (page_type_code, html_content, content_hash, http_status_code, accessed_time, url))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "# testing\n",
    "# update_page_data(\"https://bbbb.si/asdasdas/asdas\", \"HTML\", \"<head>asdasdas</head>\", \"aphufogfdfakjnlskxcj\", 200, '2022-07-01 00:00:00')\n",
    "# update_page_data(\"https://bbbb.si/page2\", \"HTML\", \"<head>page 2 bbbb</head>\", \"oaisdjoaisjdojasodj\", 200, '2022-07-01 00:00:00')\n",
    "update_page_data(\"http://bbbb.si/page2duplicate3\", \"HTML\", \"<head>page4 cccc</head>\", \"aaa\", 200, '2022-07-01 00:00:00.123456')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserting values into page as BINARY\n"
     ]
    }
   ],
   "source": [
    "# if page is binary - update the page_data or image_data tables\n",
    "def insert_page_data(url, http_status_code, accessed_time, data_type_code):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    print(\"\\nInserting values into page as BINARY\")\n",
    "    cur.execute(\"UPDATE crawldb.page SET page_type_code='BINARY', http_status_code= %s, accessed_time= %s \\\n",
    "                WHERE url= %s RETURNING id\", (http_status_code, accessed_time, url))\n",
    "    \n",
    "    # NOTE - this can cause an error if the page hasn't been inserted yet\n",
    "    id_of_updated_row = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute(\"INSERT INTO crawldb.page_data (page_id, data_type_code) VALUES (%s, %s)\", (id_of_updated_row, data_type_code))\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "# testing\n",
    "# update_page_data(\"https://bbbb.si/asdasdas/asdas\", \"HTML\", \"<head>asdasdas</head>\", \"aphufogfdfakjnlskxcj\", 200, '2022-07-01 00:00:00')\n",
    "# update_page_data(\"https://bbbb.si/page2\", \"HTML\", \"<head>page 2 bbbb</head>\", \"oaisdjoaisjdojasodj\", 200, '2022-07-01 00:00:00')\n",
    "insert_page_data(\"http://ee.si/binary2\", 200, '2022-07-01 00:00:00.123456', 'DOC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert page data v2\n",
    "def insert_page_data(url, http_status_code, accessed_time, data_type_code):\n",
    "    cur = None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # print(\"\\nInserting values into page as BINARY\")\n",
    "        cur.execute(\"UPDATE crawldb.page SET page_type_code='BINARY', http_status_code= %s, accessed_time= %s \\\n",
    "                    WHERE url= %s RETURNING id\", (http_status_code, accessed_time, url))\n",
    "        \n",
    "        # NOTE - this can cause an error if the page hasn't been inserted yet\n",
    "        id_of_updated_row = cur.fetchone()[0]\n",
    "\n",
    "        cur.execute(\"INSERT INTO crawldb.page_data (page_id, data_type_code) VALUES (%s, %s)\", (id_of_updated_row, data_type_code))\n",
    "    except Exception as e:\n",
    "        print(\"Error while inserting page data (binary page): \", e)\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserting values into 'page' as BINARY\n"
     ]
    }
   ],
   "source": [
    "# if page is an image - update the image_data table\n",
    "def insert_image_data(url, filename, content_type, accessed_time):\n",
    "    cur = None\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # print(\"\\nInserting values into 'page' as BINARY\")\n",
    "        cur.execute(\"SELECT id FROM crawldb.page WHERE url = %s\", (url,))\n",
    "        \n",
    "        # NOTE - this can cause an error if the page hasn't been inserted yet\n",
    "        page_id = cur.fetchone()[0]\n",
    "\n",
    "        cur.execute(\"INSERT INTO crawldb.image (page_id, filename, content_type, accessed_time) VALUES (%s, %s, %s, %s)\", (page_id, filename, content_type, accessed_time))\n",
    "        \n",
    "        cur.close()\n",
    "    except Exception as e:\n",
    "        print(\"Error while inserting image data: \", e)\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "\n",
    "    \n",
    "# testing\n",
    "# update_page_data(\"https://bbbb.si/asdasdas/asdas\", \"HTML\", \"<head>asdasdas</head>\", \"aphufogfdfakjnlskxcj\", 200, '2022-07-01 00:00:00')\n",
    "# update_page_data(\"https://bbbb.si/page2\", \"HTML\", \"<head>page 2 bbbb</head>\", \"oaisdjoaisjdojasodj\", 200, '2022-07-01 00:00:00')\n",
    "insert_image_data(\"http://ee.si/binary4\", 200, '2022-07-01 00:00:00.123456', 'image2.jpeg','img/jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page url\n",
      "https://bbbb.si/page3\n"
     ]
    }
   ],
   "source": [
    "# get url of first page from frontier\n",
    "def get_first_page_from_frontier():\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "\n",
    "    print(\"Getting page url\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # get url of first page with code set to FRONTIER\n",
    "    cur.execute(\"SELECT url FROM crawldb.page WHERE page_type_code = 'FRONTIER'\")\n",
    "    page_url = cur.fetchone()[0]\n",
    "    \n",
    "    print(page_url)\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "get_first_page_from_frontier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "73a61cd168f7e928ade47c9b8af7c78aaedf02402bb28f55053bc5c16ff0c5d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
