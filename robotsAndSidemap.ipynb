{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "def robots_context(text):\n",
    "    result_dict = {}\n",
    "    lines = text.split('\\n')\n",
    "    sitemaps = []\n",
    "    for line in lines:\n",
    "        if line.startswith('Sitemap:'):\n",
    "            #včasih ni napisano nič za \"Sitemap:\"\n",
    "            if len(line.split())>1: sitemaps.append(line.split()[1])\n",
    "        elif line.startswith('User-agent:') and line.split()[1] == '*':\n",
    "            disallows = []\n",
    "            allows = []\n",
    "            for agent in lines[lines.index(line)+1:]:\n",
    "                print(agent)\n",
    "                #če ni nič od naštetega v vrsici, nima smisla nadaljevati\n",
    "                if agent.strip() and  not agent.strip().startswith(('Disallow:', 'Allow:', '#')):\n",
    "                    break\n",
    "                if agent.startswith('Disallow:'):\n",
    "                    if len(agent.split())>1: disallows.append(agent.split()[1])        \n",
    "                if agent.startswith('Allow:'):\n",
    "                    if len(agent.split())>1: allows.append(agent.split()[1]) \n",
    "            result_dict['User-agent'] = {'Disallow': disallows, 'Allow': allows}\n",
    "        elif line.startswith('Crawl-delay:'):\n",
    "            if len(line.split())>1: result_dict['Crawl-delay'] = line.split()[1]\n",
    "        else:\n",
    "            continue\n",
    "    result_dict['Sitemap'] = sitemaps\n",
    "    return result_dict\n",
    "\n",
    "def main_page():\n",
    "    #vstavimo neko stran ki ni prva.\n",
    "\n",
    "    #kliče funkcijo, ki scrapa stran\n",
    "\n",
    "    #pridobi osnovno stran in vstavi dobljene info v bazo\n",
    "\n",
    "    #na izhodu dobimo:\n",
    "    #id glavne -> mogoče samo to vrne, ker se lahko kliče bazo\n",
    "    #vse strani iz sitemap\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def url_sitemap(url):\n",
    "    #za vstavlen url osnovne domene dobi vse povezave na strani\n",
    "    response = requests.get(url + '/robots.txt')\n",
    "    sitemap_list = robots_context(response.text)\n",
    "    print(sitemap_list['Sitemap'])\n",
    "    urls = []\n",
    "    for sitemap in sitemap_list['Sitemap']:\n",
    "        urls.extend(parse_sitemap(sitemap))\n",
    "    return urls\n",
    "\n",
    "\n",
    "def parse_sitemap(url_sitemap):\n",
    "    #dobi urlje od sitmap xml datoteke\n",
    "    urls = []\n",
    "\n",
    "    #če je ni\n",
    "    if url_sitemap is  None:\n",
    "        return []\n",
    "\n",
    "    # Fetch the sitemap XML file\n",
    "    response = requests.get(url_sitemap)\n",
    "    if response.status_code == 200:\n",
    "        #preoblikujemo v leš\n",
    "        soup = BeautifulSoup(response.content, 'xml')\n",
    "        \n",
    "\n",
    "    \n",
    "        #išče <sitemap>, ki nakazuje na .xml datoteke\n",
    "        sitemap = soup.find('sitemap')\n",
    "        if sitemap is not None:\n",
    "            \n",
    "            \n",
    "            for sitemap in soup.sitemapindex.find_all('sitemap'):\n",
    "                # Extract the URLs from the sitemap\n",
    "                loc = sitemap.loc.text\n",
    "                \n",
    "                #print(f'Location: {loc}')\n",
    "\n",
    "                #če so xml datoteke\n",
    "                if loc.endswith(\".xml\") or loc.endswith(\".XML\"):\n",
    "                    urls.extend(parse_sitemap(loc))\n",
    "                    \n",
    "            #kliče sebe s parametrom, ki vsebuje ime xml datoteke z url-ji\n",
    "        \n",
    "        else:\n",
    "            #datotka vsebuje url-je, išče <urlset>\n",
    "            urlset = soup.find('urlset')\n",
    "            if urlset is not None:\n",
    "                url = soup.find('url')\n",
    "                if url is not None:\n",
    "                    urls.extend([element.text for element in soup.find_all('loc')])\n",
    "       \n",
    "  \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pomembno ker nekateri strani brez tega ne dovolijo dostop!\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "url = 'https://okusno.je/'  # Replace example.com with the actual domain\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code == 200:  # Check if the response is successful\n",
    "    #print(len(url_sitemap(url))) #strani je preveč da bi jih editor izpisal zato len\n",
    "    resp = requests.get(url + '/robots.txt', headers=headers)\n",
    "    print(robots_context(resp.text))\n",
    "else:\n",
    "    print('Failed to retrieve robots.txt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
